import json

from templates.prompting import CTFSolvePrompt, few_Shot
from utility.core_utility import Core
from openai import OpenAI
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# 새로운 google-genai SDK 사용
try:
    from google import genai
    from google.genai import types
except ImportError:
    genai = None
    types = None

core = Core()

class ExploitAgent:
    def __init__(self, api_key: str, model: str = "gpt-5.2"):
        self.api_key = api_key
        self.model = model

        if model == "gpt-5.2":
            self.client = OpenAI(api_key=api_key)
            self.is_gemini = False
        elif model in ["gemini-1.5-flash", "gemini-1.5-flash-latest", "gemini-3-flash-preview"]:
            if genai is None:
                raise ImportError("google-genai package is required for Gemini. Install with: pip install google-genai")
            self.client = genai.Client(api_key=api_key)
            self.gemini_model = model
            self.is_gemini = True
        else:
            raise ValueError(f"Invalid model: {model}. Supported: gpt-5.2, gemini-1.5-flash, gemini-1.5-flash-latest, gemini-3-flash-preview")

    def _call_gemini(self, system_instruction: str, user_content: str):
        from google.genai import types
        from rich.console import Console
        console = Console()

        config = types.GenerateContentConfig(
            system_instruction=system_instruction if system_instruction else None,
        )

        response = self.client.models.generate_content(
            model=self.gemini_model,
            contents=user_content,
            config=config
        )

        # 디버깅: 응답 상태 출력
        if response is None:
            console.print("  [API] Gemini returned None", style="bold red")
            return '{"error": "Gemini returned None"}'

        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'finish_reason'):
                finish_reason = str(candidate.finish_reason)
                if 'SAFETY' in finish_reason.upper() or 'BLOCK' in finish_reason.upper():
                    console.print(f"  [API] Response blocked: {finish_reason}", style="bold red")
                    return '{"error": "blocked"}'

        text = response.text
        if not text or text.strip() == "":
            console.print("  [API] Empty response from Gemini", style="bold yellow")
            return '{"error": "empty response"}'

        console.print(f"  [API] Response OK ({len(text)} chars)", style="dim green")
        return text

    def exploit_run(self, prompt_query: str, available_tools: list = None):
        state = core.load_json("state.json", default="")

        # Heap exploitation knowledge 추가
        fewshot = few_Shot()
        heap_knowledge = getattr(fewshot, 'heap_exploitation_knowledge', '')

        # prompt_query나 state에서 heap 관련 키워드 감지
        check_text = prompt_query.lower() + " " + json.dumps(state).lower()
        heap_keywords = ['heap', 'malloc', 'free', 'uaf', 'use-after-free', 'double free', 'tcache', 'unsorted', 'fastbin']
        is_heap_challenge = any(kw in check_text for kw in heap_keywords)

        exploit_prompt_with_heap = CTFSolvePrompt.exploit_prompt
        if heap_knowledge and is_heap_challenge:
            exploit_prompt_with_heap = CTFSolvePrompt.exploit_prompt + "\n\n" + heap_knowledge
        
        # AVAILABLE_TOOLS 정보 추가
        tools_info = ""
        if available_tools:
            tools_info = (
                "\n### AVAILABLE_TOOLS:\n{tools}\n\n"
                "If you need to use one_gadget, check if 'one_gadget_search' is in AVAILABLE_TOOLS.\n"
                "If it exists, use the tool instead of subprocess.\n\n"
            ).format(tools=json.dumps(available_tools, indent=2, ensure_ascii=False))
        
        if tools_info:
            exploit_prompt_with_heap = exploit_prompt_with_heap + "\n\n" + tools_info

        prompt_exploit = [
            {"role": "developer", "content": exploit_prompt_with_heap},
        ]

        state_msg = {"role": "developer", "content": "[STATE]\n" + json.dumps(state, ensure_ascii=False)}
        user_msg = {"role": "user", "content": prompt_query}

        call_msgs = prompt_exploit + [state_msg, user_msg]

        if self.is_gemini:
            # 새로운 google-genai SDK 사용
            system_parts = []
            user_parts = []

            for msg in call_msgs:
                role = msg.get("role", "user")
                content_text = msg.get("content", "")
                if role == "developer" or role == "system":
                    system_parts.append(content_text)
                elif role == "user":
                    user_parts.append(content_text)

            system_instruction = "\n\n".join(system_parts) if system_parts else None
            user_content = "\n\n".join(user_parts) if user_parts else ""

            return self._call_gemini(system_instruction, user_content)
        else:
            res = self.client.chat.completions.create(model=self.model, messages=call_msgs)
            return res.choices[0].message.content

    def poc_run(self, prompt_query: str, available_tools: list = None, one_gadget_info: dict = None):
        # Heap exploitation knowledge for PoC generation
        fewshot = few_Shot()
        heap_knowledge = getattr(fewshot, 'heap_exploitation_knowledge', '')

        # AVAILABLE_TOOLS 정보 추가
        tools_info = ""
        if available_tools:
            tools_info = (
                "\n### AVAILABLE_TOOLS:\n{tools}\n\n"
                "If you need to use one_gadget, check if 'one_gadget_search' is in AVAILABLE_TOOLS.\n"
                "If it exists, use the tool instead of subprocess.\n\n"
            ).format(tools=json.dumps(available_tools, indent=2, ensure_ascii=False))

        # ONE_GADGET_INFO 정보 추가 (자동 탐지된 경우)
        one_gadget_context = ""
        if one_gadget_info and one_gadget_info.get("offsets"):
            offsets = one_gadget_info["offsets"]
            one_gadget_context = (
                "\n### PRE-COMPUTED ONE_GADGET OFFSETS (USE THESE DIRECTLY) ###\n"
                "The system has already extracted one_gadget offsets from the actual libc.\n"
                "DO NOT run subprocess or hardcode different values. Use these directly:\n\n"
                "```python\n"
                "# Pre-computed from actual libc: {libc_path}\n"
                "ONE_GADGETS = [\n"
            ).format(libc_path=one_gadget_info.get("libc_path", "unknown"))

            for gadget in offsets:
                addr = gadget.get("address", "")
                constraint = gadget.get("constraints", "")[:60]
                one_gadget_context += f"    {addr},  # {constraint}...\n"

            one_gadget_context += (
                "]\n"
                "# Use: one_gadget_addr = libc_base + ONE_GADGETS[i]\n"
                "```\n\n"
            )

        # Check for heap-related content in prompt
        check_text = prompt_query.lower()
        heap_keywords = ['heap', 'malloc', 'free', 'uaf', 'use-after-free', 'double free', 'tcache', 'unsorted', 'fastbin', 'libc']
        is_heap_challenge = any(kw in check_text for kw in heap_keywords)

        poc_prompt_content = CTFSolvePrompt.poc_prompt
        if heap_knowledge and is_heap_challenge:
            poc_prompt_content = CTFSolvePrompt.poc_prompt + "\n\n" + heap_knowledge

        # ONE_GADGET_INFO 정보를 먼저 추가 (최우선)
        if one_gadget_context:
            poc_prompt_content = poc_prompt_content + "\n\n" + one_gadget_context

        # AVAILABLE_TOOLS 정보를 프롬프트에 추가
        if tools_info:
            poc_prompt_content = poc_prompt_content + "\n\n" + tools_info

        prompt_poc = [
            {"role": "developer", "content": poc_prompt_content},
        ]

        user_msg = {"role": "user", "content": prompt_query}

        call_msgs = prompt_poc + [user_msg]

        if self.is_gemini:
            # 새로운 google-genai SDK 사용
            system_parts = []
            user_parts = []

            for msg in call_msgs:
                role = msg.get("role", "user")
                content_text = msg.get("content", "")
                if role == "developer" or role == "system":
                    system_parts.append(content_text)
                elif role == "user":
                    user_parts.append(content_text)

            system_instruction = "\n\n".join(system_parts) if system_parts else None
            user_content = "\n\n".join(user_parts) if user_parts else ""

            return self._call_gemini(system_instruction, user_content)
        else:
            res = self.client.chat.completions.create(model=self.model, messages=call_msgs)
            return res.choices[0].message.content
